{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8aace5-f6ae-490c-854e-3a8b8e5e3f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384bc04164e64315a7acaa3c757b41ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "llm = \"NousResearch/Nous-Hermes-2-Mistral-7B-DPO\"\n",
    "llm_name = llm.replace('/','_').replace('-','_')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm,\n",
    "    torch_dtype=torch.float16,\n",
    "  \n",
    ")\n",
    "model =model.to('cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm,use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53817d-5ef6-46e6-800a-eadfd8332d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415f84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LLM_summeval' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# clone this repo to have data\n",
    "!git clone https://github.com/DAMO-NLP-SG/LLM_summeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b93eaa-5f6c-46f1-ae5a-39e611b8b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers tqdm pandas numpy\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31592ec9-91c0-4f99-b220-6d7792c4caaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50052198-edcb-4930-ae24-31fd7286ede0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb11964-b194-40c8-8de8-7250c1f9e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls LLM_summeval/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcda0c29-e0c2-46b9-afa2-b1b89dba3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_mapping={0:'relevance',1:'consistency',2:'fluency',3:'coherence'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14231094-bfb0-44c7-aff4-87d82fbf3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompts from LLM_summeval repository https://github.com/DAMO-NLP-SG/LLM_summeval\n",
    "def prepare_mcq_prompt2(aspect_id, summary, article=None, order = [0,1,2,3,4]):\n",
    "    if aspect_id == 0:\n",
    "        scores = 'A: The Summary is totally irrelevant to the Article. Score: One.\\nB: The majority of the Summary is irrelevant to the Article. Score: Two.\\nC: Some information in the Summary is relevant to the Article whereas some are not. Score: Three.\\nD: The majority of the Summary is relevant to the Article. Score: Four.\\nE: All information included in the Summary is relevant to the Article. Score: Five.\\n\\nYour Answer (enter 1 letter from A to E):'\n",
    "        prompt = f'Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to relevance from one to five, where one indicates \"irrelevance\", and five indicates \"perfect relevance\". Note that relevance measures the Summary\\'s selection of important content from the Article, whether the Summary grasps the main message of the Article without being overwhelmed by unnecessary or less significant details.\\n\\nArticle: {article}\\n\\nSummary: {summary}\\n\\n'\n",
    "        \n",
    "\n",
    "    if aspect_id == 1:\n",
    "        scores = 'A: The Summary is totally inconsistent with the Article. Score: One.\\nB: The majority of the Summary is inconsistent with the Article. Score: Two.\\nC: Some information in the Summary is consistent with the Article whereas some are not. Score: Three.\\nD: The majority of the Summary is consistent with the Article. Score: Four.\\nE: All information included in the Summary is consistent with the Article. Score: Five.\\n\\nYour Answer (enter 1 letter from A to E):'\n",
    "        prompt = f'Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to consistency from one to five, where one indicates \"inconsistency\" and five indicates \"perfect consistency\". Note that consistency measures the factual alignment between the Summary and the Article, whether the Summary is faithful to the Article without introducing contradictions or misleading representations.\\n\\nArticle: {article}\\n\\nSummary: {summary}\\n\\n'\n",
    "\n",
    "\n",
    "    if aspect_id == 2: \n",
    "        scores = 'A: The Summary is totally disfluent. Score: One.\\nB: The majority of the Summary is disfluent. Score: Two.\\nC: Some sentences in the Summary are fluent whereas some are not. Score: Three.\\nD: The majority of the Summary is fluent. Score: Four\\nE: All sentences in the Summary are fluent. Score: Five.\\n\\nYour Answer (enter 1 letter from A to E):'\n",
    "        prompt = f'Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to fluency from one to five, where one indicates \"disfluency\" and five indicates \"perfect fluency\". Note that fluency measures the quality of individual sentences in the Summary, whether the Summary is well-written, grammatically correct, and readable on the sentence level.\\n\\nArticle: {article}\\n\\nSummary: {summary}\\n\\nA: The Summary is totally disfluent. Score: One.\\nB: The majority of the Summary is disfluent. Score: Two.\\nC: Some sentences in the Summary are fluent whereas some are not. Score: Three.\\nD: The majority of the Summary is fluent. Score: Four\\nE: All sentences in the Summary are fluent. Score: Five.\\n\\nYour Answer (enter 1 letter from A to E):'\n",
    "    \n",
    "    if aspect_id == 3:\n",
    "        scores = 'A: The Summary is completely incoherent. Score: One.\\nB: The Summary is mostly incoherent. Score: Two.\\nC: The Summary is somewhat coherent. Score: Three.\\nD: The Summary is mostly coherent. Score: Four.\\nE: The Summary is completely coherent. Score: Five.\\n\\nYour Answer (enter 1 letter from A to E):'\n",
    "        prompt = f'Choose an option from A to E in order to score the following Summary given the corresponding Article with respect to coherence from one to five, where one indicates \"incoherence\" and five indicates \"perfect coherence\". Note that coherence measures the collective quality of the Summary, whether the Summary presents information that flows smoothly and avoids abrupt transitions or disjoint statements.\\n\\nArticle: {article}\\n\\nSummary: {summary}\\n\\nA: The Summary is completely incoherent. Score: One.\\nB: The Summary is mostly incoherent. Score: Two.\\nC: The Summary is somewhat coherent. Score: Three.\\nD: The Summary is mostly coherent. Score: Four.\\nE: The Summary is completely coherent. Score: Five.\\n\\nYour Answer (enter 1 letter from A to E):'\n",
    "\n",
    "    scores = scores.split('\\n')\n",
    "    scores = [scores[j] for j in order]\n",
    "    return prompt + '\\n'.join(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a427be8f-d55c-42cc-bce7-cb9a5b4a69f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "data=json.loads(\n",
    "pathlib.Path('LLM_summeval/summeval.json').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32745686-fdd5-4553-8472-a544865be5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26ae4db6-4867-4d0f-a262-8ba2ea9013a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_json(data):\n",
    "    keys2 = ['src', 'ref_summs', 'sys_summs', 'ref_summ']\n",
    "    article = data[keys[0]]['src']\n",
    "    \n",
    "    records = []\n",
    "    for k,d in data.items():\n",
    "        summaries = d['sys_summs']\n",
    "        article = d['src']\n",
    "    \n",
    "        for k1, item in list(summaries.items()):\n",
    "            record = {\n",
    "            }\n",
    "            record['article'] = article\n",
    "            record['summary'] = item['sys_summ']\n",
    "            record['key'] = k\n",
    "            record['key2'] = k1\n",
    "            for k in ['relevance', 'consistency', 'fluency', 'coherence']:\n",
    "                record[k] = item['scores'][k]\n",
    "                record['chatgpt_'+k] = item['scores']['chatgpt_'+k]\n",
    "            records.append(record)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026b320-0874-4759-be6e-ab4580806351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41968638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re, numpy as np\n",
    "\n",
    "value_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5}\n",
    "TOP_N_TOKENS = 20\n",
    "def process_prompt(text):\n",
    "    messages = [      \n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]   \n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    return model_inputs\n",
    "def filter_token(tok):\n",
    "    # remove punctuation\n",
    "    tok = re.sub(r'[^\\w\\s]','',tok)\n",
    "    return tok\n",
    "def process_logits(model_out, T=10):\n",
    "\n",
    "    logits = model_out.logits[:,-1,:].cpu().numpy()\n",
    "    logits = logits/T\n",
    "    top_args = logits.argsort()[0,-TOP_N_TOKENS:]\n",
    "\n",
    "    labels = tokenizer.convert_ids_to_tokens(top_args)\n",
    "    logits_chosen = np.exp(logits[0,top_args] - logits[0,top_args].max())\n",
    "    logits_chosen = logits_chosen / logits_chosen.sum()\n",
    "\n",
    "    #print(logits_chosen, labels)\n",
    "\n",
    "    logits_chosen2 = [(filter_token(l),p) for (l,p) in list(zip(labels,logits_chosen)) if filter_token(l) in 'ABCDE' and filter_token(l) != '']\n",
    "    #print(logits_chosen2)\n",
    "    sum_logits = sum([p for l,p in logits_chosen2])\n",
    "    logits_scaled = [(value_map.get(l),p/sum_logits) for l,p in logits_chosen2]\n",
    "\n",
    "    argmax = max(logits_scaled, key=lambda x: x[1])[0]\n",
    "    expected_value = sum([l*p for l,p in logits_scaled])\n",
    "    return argmax, expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [17:42<00:00,  1.51it/s]\n",
      "100%|██████████| 1600/1600 [17:35<00:00,  1.52it/s]\n",
      " 18%|█▊        | 288/1600 [03:37<15:34,  1.40it/s]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import copy\n",
    "import numpy as np\n",
    "torch.no_grad()\n",
    "order = [0,1,2,3,4]\n",
    "\n",
    "for idx in [0,1,2,3]:\n",
    "    idx_metric=idx\n",
    "    for record in tqdm.tqdm(records[0:1600]):\n",
    "        for ord_ in ['inorder','reverse','random']:\n",
    "            ord_arr = {'inorder':order, 'reverse':order[::-1],'random':copy.copy(order)}[ord_]\n",
    "            if ord_ == 'random':\n",
    "                np.random.shuffle(ord_arr)\n",
    "            txt = prepare_mcq_prompt2(idx, record['summary'], record['article'], ord_arr)\n",
    "            model_inputs = process_prompt(txt)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_out = model(model_inputs.input_ids)\n",
    "                argmax, expected_value = process_logits(model_out, T=10)\n",
    "                argmax2, expected_value2 = process_logits(model_out, T=1)\n",
    "                record['argmax_'+ord_+'_' + key_mapping[idx_metric]] = argmax\n",
    "                record['e(s)_10_' +ord_+'_' + key_mapping[idx_metric]] = expected_value\n",
    "                record['e(s)_1_' +ord_+'_' + key_mapping[idx_metric]] = expected_value2\n",
    "                del model_out\n",
    "\n",
    "    import json\n",
    "    with open(f'processed_data_{llm_name}_{idx}.json','w') as fp:\n",
    "        json.dump(records,fp,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2db5e16-b534-4341-99b0-f3101b6427e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'processed_data_{llm_name}.json','w') as fp:\n",
    "    json.dump(records,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c14f7d-6cfd-4713-be29-d3d6a8fcb9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fd72dfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expected_10_relevance\n",
       "1.0    1600\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61ee9a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc5671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3fb395f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b84bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10978aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
